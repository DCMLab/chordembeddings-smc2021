{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicition_continual\n",
    "This notebook details the pipeline for continual next-chord prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data \n",
    "When loading the chord dataset, we can choose whether to keep sections in major or minor key, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import load_train_test_sentences, all_composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Choose which composers to train on and which to test on\n",
    "composers = all_composers\n",
    "test_composers = ['Pleyel']\n",
    "\n",
    "train_sentences, test_sentences, _ = load_train_test_sentences(composers, test_composers, key_mode='MAJOR')\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Word2Vec\n",
    "Several hyperparameters to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from load_data import get_chord_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore words with a lower frequency frequency than this\n",
    "min_count = 50\n",
    "# Size of the embedding space\n",
    "size = 5\n",
    "# Neighborhood of the focus word to study\n",
    "window = 2\n",
    "# 0 for CBOW, 1 for skip-gram\n",
    "sg = 1\n",
    "# Number of iterations (epochs)\n",
    "iter = 500\n",
    "\n",
    "# The first argument has to be a list of lists of words\n",
    "w2v_model = Word2Vec(train_sentences, min_count=min_count, size=size, window=window, sg=sg, iter=iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['I:MAJ', 'V:MAJ', 'IV:MAJ', '#IV:DIM', 'II:MAJ', 'VI:MIN', 'bVII:MAJ', 'VII:DIM', 'III:MAJ', 'VI:MAJ', 'II:MIN', '#I:DIM', 'V:MIN', 'III:DIM', 'II:DIM', 'IV:MIN', '#V:DIM', 'VII:MAJ', 'III:MIN', 'I:MIN', 'VII:MIN', 'bIII:MAJ', 'bVI:MAJ', '#II:DIM', 'VI:DIM', 'I:AUG', 'V:AUG', 'bVII:MIN', 'bII:MAJ', '#IV:MAJ', 'V:DIM', '#I:MAJ'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Train the LSTM predictor on the same dataset as the Word2Vec model, then test it on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_continual import LSTMPredictor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anzuoni Elia\\Documents\\EPFL\\Machine Learning\\Project2\\lstm_continual.py:27: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  embed = torch.tensor(self.wv[chord])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000 : average loss = 2.5826123503684997\n",
      "Iteration 10000 : average loss = 2.659360311770439\n",
      "Iteration 15000 : average loss = 2.4755226123332976\n",
      "Iteration 20000 : average loss = 2.308939964568615\n",
      "Iteration 25000 : average loss = 2.393791701543331\n",
      "Iteration 30000 : average loss = 2.108278911948204\n",
      "Iteration 35000 : average loss = 1.7450329397380353\n",
      "Iteration 40000 : average loss = 1.948295321702957\n",
      "Iteration 45000 : average loss = 2.162517180353403\n",
      "Iteration 50000 : average loss = 2.523907002145052\n",
      "Iteration 55000 : average loss = 1.9201798030078412\n",
      "Iteration 60000 : average loss = 1.6977726721227169\n",
      "Iteration 65000 : average loss = 2.1149688821807504\n",
      "Closing epoch 0 \n",
      "\n",
      "Starting epoch 1\n",
      "Iteration 5000 : average loss = 2.299036927694082\n",
      "Iteration 10000 : average loss = 2.607781600686908\n",
      "Iteration 15000 : average loss = 2.3695591732859613\n",
      "Iteration 20000 : average loss = 2.0840927137702705\n",
      "Iteration 25000 : average loss = 2.243418509307504\n",
      "Iteration 30000 : average loss = 1.9323996920526028\n",
      "Iteration 35000 : average loss = 1.5192366954408587\n",
      "Iteration 40000 : average loss = 1.7400522111743688\n",
      "Iteration 45000 : average loss = 2.0204725423783065\n",
      "Iteration 50000 : average loss = 2.437066127228737\n",
      "Iteration 55000 : average loss = 1.7640424709796905\n",
      "Iteration 60000 : average loss = 1.5474002314448356\n",
      "Iteration 65000 : average loss = 2.0497284945756196\n",
      "Closing epoch 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm_predictor = LSTMPredictor(w2v_model, hidden_dim=4)\n",
    "optimiser = optim.Adam(lstm_predictor.parameters(), lr=0.001)\n",
    "\n",
    "# Training (the method 'train' was already taken)\n",
    "lstm_predictor.learn(train_sentences, optimiser, 2)\n",
    "# Training takes a couple minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 0.6432160804020101\n",
      "Accuracy by chord\n",
      " {'V:MAJ': 0.9685863874345549, 'I:MAJ': 0.8796296296296297, 'IV:MAJ': 0.034482758620689655, 'II:MIN': 0.0, 'VI:MIN': 0.2916666666666667, 'VII:DIM': 0.0, 'III:MIN': 0.0, 'I:MIN': 0.0, 'VI:MAJ': 0.0, '#IV:DIM': 0.0, 'II:MAJ': 0.0, 'IV:MIN': 0.0, 'III:MAJ': 0.0, '#II:DIM': 0.0, '#I:DIM': 0.0, 'I:AUG': 0.0, '#V:DIM': 0.0, 'VI:DIM': 0.0, 'III:DIM': 0.0, 'II:DIM': 0.0}\n",
      "Occurrences by chord\n",
      " {'V:MAJ': 191, 'I:MAJ': 216, 'IV:MAJ': 58, 'II:MIN': 39, 'VI:MIN': 24, 'VII:DIM': 20, 'III:MIN': 2, 'I:MIN': 4, 'VI:MAJ': 7, '#IV:DIM': 6, 'II:MAJ': 5, 'IV:MIN': 1, 'III:MAJ': 4, '#II:DIM': 3, '#I:DIM': 8, 'I:AUG': 2, '#V:DIM': 4, 'VI:DIM': 1, 'III:DIM': 1, 'II:DIM': 1}\n"
     ]
    }
   ],
   "source": [
    "accuracy_total, accuracy_by_chord, occurrences_by_chord = lstm_predictor.test(test_sentences)\n",
    "\n",
    "print('Total accuracy:', accuracy_total)\n",
    "print('Accuracy by chord\\n', accuracy_by_chord)\n",
    "print('Occurrences by chord\\n', occurrences_by_chord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
